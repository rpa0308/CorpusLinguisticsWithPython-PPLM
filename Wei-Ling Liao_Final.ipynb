{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<h1>Final Project: Positive Pointwise Mutual Information</h2>\n",
    "<h2>Corpus Linguistics with Python - Summer Semester 2022</h1>\n",
    "<h3>Wei-Ling Liao - Matriculation Nr. 1194717.</h3>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Corpus Pre-Processing\n",
    "The following code cell pre-process the 'text_fic.txt'. First, I import the NLTK and regular expression modules and assign the input and output files to the corresponding directory. I define a tag_set because NLTK provides a simple lemmatizer that recognizes and processes **content words only**: nouns, verbs, adjectives, and adverbs. By default, the lemmatizer treats each word as a noun. Therefore, tag_set is for recognizing other parts of speech than nouns.\n",
    "\n",
    "Description of code: I investigated the my_coca.preprocessed and found out that 'paragraph element' were removed; therefore, I did the same thing. Next, I use sent_tokenize to identify sentence boundaries. For each sentence, I use word_tokenize to split the words. Every token is assigned a pos and the WordNetLemmatizer is instantiated. Next, the code iterates each token ,lowers the words and clarifies the pos. Then, tokens are lemmatized according to their pos and they are appended into a list which is then combined by space-seperated string using join function. In the end, sentences are seperated by sent_tokenize and print to coca_preprocessed file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk import sent_tokenize\n",
    "from nltk import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "fn = \"/compLing/students/courses/corpusLinguistics/finalProject/text_fic.txt\"\n",
    "fnout = \"/users/wei-ling.liao/CLPython/FinalProject/coca.preprocessed\"\n",
    "tag_set = {\"J\" : \"a\", \"N\" : \"n\", \"V\" : \"v\", \"R\" : \"r\", \"M\": \"v\"} #Penn Treebank Tagsets\n",
    "lemmaList = []\n",
    "\n",
    "with open(fn, 'r') as f, open(fnout, 'w') as fout:\n",
    "    for line in f:\n",
    "        pattern = re.compile(r\"<p>\")\n",
    "        matching = pattern.sub(r\"\", line)\n",
    "        \n",
    "        sentences = sent_tokenize(matching)\n",
    "        for sentence in sentences:\n",
    "            tokens = word_tokenize(sentence)\n",
    "            postags = pos_tag(tokens)\n",
    "            lemmatizer = WordNetLemmatizer()\n",
    "            \n",
    "            for w in range(len(tokens)):\n",
    "                token = tokens[w].lower()\n",
    "                pos = postags[w][1]\n",
    "                firstPOSletter = pos[0]\n",
    "                \n",
    "                if (firstPOSletter in tag_set) and (pos != \"NFP\"): #NFP: pos of punctuations\n",
    "                    lemma = lemmatizer.lemmatize(token, pos = tag_set[firstPOSletter])\n",
    "                else:\n",
    "                    lemma = token         \n",
    "                    \n",
    "                lemmaList.append(lemma)\n",
    "        \n",
    "    lemmatized_text = \" \".join(lemmaList)\n",
    "    sentences = sent_tokenize(lemmatized_text)\n",
    "    for sentence in sentences:\n",
    "        print(sentence, file = fout)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Counting Unigrams and Bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I chose to use my_coca.preprocessed as an input file to avoid wrong output for the following steps because my file from pre-processing is not exactly the same as the one we were given for this project.\n",
    "\n",
    "Description of code: To compute the frequency of unigrams and bigrams, two dictionaries are created. First, I tokenize the words with the word_tokenize function and assign them to the variable 'tokens'. Next, I extract the bigrams and store them in a list. Now the dictionaries can be filled seperately with similar iteration procedure. As requested, both in unigramsDict and bigramsDict, frequency <=5 is set to 0. In the end, unigramsDict is used to count the total number of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of lemmas: 1372584.0\n"
     ]
    }
   ],
   "source": [
    "from nltk import bigrams\n",
    "\n",
    "fn = \"/compLing/students/courses/corpusLinguistics/finalProject/outputFiles/my_coca.preprocessed\"\n",
    "fnout = \"/users/wei-ling.liao/CLPython/FinalProject/coca.counts\"\n",
    "\n",
    "unigramsDict = {}\n",
    "bigramsDict = {}\n",
    "lemma = 0\n",
    "\n",
    "with open(fn, \"r\") as f, open(fnout, \"w\") as fout:\n",
    "    for line in f:\n",
    "        tokens = word_tokenize(line)\n",
    "        bigramsList = list(bigrams(tokens))\n",
    "   \n",
    "        # fill the unigramDict\n",
    "        for word in tokens:\n",
    "            if word in unigramsDict.keys():\n",
    "                unigramsDict[word] += 1\n",
    "            else:\n",
    "                unigramsDict[word] = 1\n",
    "            \n",
    "        # fill the bigramsDict\n",
    "        for bigram in bigramsList:\n",
    "            if bigram in bigramsDict.keys():\n",
    "                bigramsDict[bigram] += 1\n",
    "            else:\n",
    "                bigramsDict[bigram] = 1\n",
    "            \n",
    "    # change freq if less or equal to 5 - unigramsDict\n",
    "    for k,v in unigramsDict.items():\n",
    "        if v <= 5:\n",
    "            unigramsDict[k] = 0.0\n",
    "        print(k, v, file = fout, sep = \"\\t\")\n",
    "            \n",
    "    # change freq if less or equal to 5 - bigramsDict\n",
    "    for k,v in bigramsDict.items():\n",
    "        if v <= 5:\n",
    "            bigramsDict[k] = 0.0\n",
    "        print(k, v, file = fout, sep = \"\\t\")\n",
    "\n",
    "    # count the lemma\n",
    "    for v in unigramsDict.values():\n",
    "        lemma += v\n",
    "    print(\"Total number of lemmas:\", lemma)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: PPMI\n",
    "\n",
    "Description of code: This function takes five arguments: word1, word2, unigramsDict, bigramsDict and totalLemma. First, it is necessary to check if the words or bigram exist in the two dictionaries; if it is not in the dictionary, the frequency of the word/bigram is set to 0. Next, to avoid the failure of code execution, PMI is set to 0, if frequency of word1, word2 or bigram is 0. On the other hand, if frequency of word1, word2 and bigram are not 0, PMI is calculated according to the formula and is rounded to the 3rd decimal position. Last point, negative PMI is excluded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def PPMI(word1, word2, unigramsDict, bigramsDict, totalLemma):\n",
    "    \n",
    "    # check if the words/bigram exist\n",
    "    if word1 in unigramsDict:\n",
    "        freq1 = unigramsDict[word1]\n",
    "    else:\n",
    "        freq1 = 0\n",
    "    \n",
    "    if word2 in unigramsDict:\n",
    "        freq2 = unigramsDict[word2]\n",
    "    else:\n",
    "        freq2 = 0\n",
    "    \n",
    "    if bigram in bigramsDict:\n",
    "        bigramFreq = bigramsDict[bigram]\n",
    "    else:\n",
    "        bigramFreq = 0\n",
    "        \n",
    "    # probability - if word1 or word2 or bigrams frequency is 0 --> PPMI = 0\n",
    "    if freq1 == 0 or freq2 == 0:\n",
    "        PMI = 0\n",
    "    elif bigramFreq == 0:\n",
    "        PMI = 0\n",
    "    else:\n",
    "        word1Probability = freq1 / totalLemma\n",
    "        word2Probability = freq2 / totalLemma\n",
    "        bigramsProbability = bigramFreq / totalLemma\n",
    "    \n",
    "        #calculate PMI\n",
    "        PMI = math.log2(bigramsProbability / (word1Probability * word2Probability))\n",
    "        PMI = round(PMI, 3)\n",
    "    \n",
    "        # exclude negative PMI\n",
    "        if PMI <0:\n",
    "            PMI = 0\n",
    "    \n",
    "    # return a set of bigrams with its PPMI\n",
    "    return PMI\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Computing PPMI Scores\n",
    "Description of code: To compute PPMI of each bigram, the PPMI function is called for each bigram stored in bigramsDict. Besides, ppmiDict, where bigram is the key and ppmi score is the value, is created. Keys and values are printed in tab-separated file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnout = \"/users/wei-ling.liao/CLPython/FinalProject/coca.ppmi\"\n",
    "ppmiDict = dict()\n",
    "\n",
    "for bigram in bigramsDict:\n",
    "    word1 = bigram[0]\n",
    "    word2 = bigram[1]\n",
    "    \n",
    "    with open (fnout, \"a\") as fout: \n",
    "        ppmiScore = PPMI(word1, word2, unigramsDict, bigramsDict, totalLemma)\n",
    "        ppmiDict[bigram] = ppmiScore  #complete the dictionary\n",
    "        print(word1, word2, ppmiScore, sep = \"\\t\", file = fout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description of code: Function topN require two arguments: a dictionary and n indicating numbers of bigrams to visulize. In this case, ppmiDict from above is used because bigrams and ppmi scores are stored here. The ppmiDict is sorted by **sorted** function. To sort the dictionary, the key argument of sorted function needs to be clarified. reverse is set to True, so the value is in descending order. Lastly, the top 20 of the dictionary is printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "guiseppi\tscapellini\t17.845\n",
      "sint\tholo\t17.845\n",
      "vito\tadamo\t17.43\n",
      "palo\talto\t17.26\n",
      "tel\taviv\t17.26\n",
      "edith\tschermerhorn\t17.108\n",
      "oswald\ttruxa\t17.015\n",
      "chiang\tmai\t16.971\n",
      "macy\tlevitt\t16.971\n",
      "cecily\tscriber\t16.916\n",
      "amos\tholt\t16.845\n",
      "irene\tlashman\t16.845\n",
      "pheasant\ttheodora\t16.845\n",
      "del\tnorte\t16.73\n",
      "anarchist\tno.l\t16.623\n",
      "nil\tspaar\t16.43\n",
      "slo\tmo\t16.43\n",
      "beverly\thills\t16.371\n",
      "moyshe\trabeynu\t16.343\n",
      "artful\tdodger\t16.182\n"
     ]
    }
   ],
   "source": [
    "def topN(ppmiScores, n = 20):\n",
    "    ppmiScores = sorted(ppmiScores.items(), key = lambda x: x[1], reverse = True)\n",
    "    \n",
    "    for k, v in ppmiScores[:n]:\n",
    "        print(k[0], k[1], v, sep = \"\\t\")\n",
    "        \n",
    "topN(ppmiDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Comparing PPMI and Frequency Counts\n",
    "The topN is called to show the top20 bigrams sorted by the frequency counts. \n",
    "\n",
    "Difference between PPMI and frequency counts: For the bigram frequency counts, it gives a overview about how often does the word appear with each other. From the result under, the top 20 highest bigram frequency counts are mostly function words and punctuations, which doesn't give a clue for the content. On the other hand, PPMI considers the probability of words occurring together and the number of appearences of each token. This reduces for PPMi value for very frequent collocations (i.e., punctuations), and give a better understanding in terms of content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@\t@\t62278\n",
      ",\tand\t7211\n",
      ",\t``\t6486\n",
      ".\t``\t5934\n",
      "it\tbe\t4864\n",
      "of\tthe\t4618\n",
      "do\tnot\t4450\n",
      "in\tthe\t4406\n",
      "``\t``\t3822\n",
      "be\ta\t3416\n",
      "``\ti\t3413\n",
      "i\tbe\t3361\n",
      ",\tbut\t3144\n",
      "on\tthe\t2786\n",
      "he\tbe\t2756\n",
      "to\tthe\t2746\n",
      "be\tnot\t2690\n",
      ",\tthe\t2591\n",
      ",\ti\t2412\n",
      "say\t.\t2219\n"
     ]
    }
   ],
   "source": [
    "topN(bigramsDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
